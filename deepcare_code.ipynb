{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq97r7tsiBzl"
      },
      "source": [
        "# **Deep Care - Mining Hospital Records for Predicting Patient Drop-off**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw8NSLwt4sG3"
      },
      "source": [
        "#README\n",
        "\n",
        " Setting the PySpark Environment following executions have been done -\n",
        "\n",
        "▶ !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "▶ !pip install pyspark\n",
        "\n",
        "▶ os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl-3Q0eHeAr2"
      },
      "outputs": [],
      "source": [
        "import os                                                                                                                 # For working with file paths\n",
        "from pyspark.sql import SparkSession                                                                                      # To create and manage a SparkSession\n",
        "from pyspark.sql.functions import col, when, mean, stddev, rand, udf                                                      # Common DataFrame functions for data manipulation\n",
        "from pyspark.ml.feature import StringIndexer, Imputer, OneHotEncoder, StandardScaler, VectorAssembler                     # To convert categorical string columns into numerical indices\n",
        "from pyspark.sql.functions import array                                                                                   # To work with arrays in DataFrames\n",
        "from pyspark.ml.clustering import KMeans                                                                                  # KMeans clustering algorithm from MLlib\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator   # For evaluating clustering models\n",
        "from pyspark.sql.functions import col, count, countDistinct, when, max as spark_max, to_timestamp                         # Compute maximum value; aliased as spark_max to avoid conflicts with built-in max()\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier                           # Import classification models\n",
        "from pyspark.ml.linalg import VectorUDT                                                                                   # Import for vector handling in custom UDFs (User Defined Functions)\n",
        "from pyspark.sql.types import DoubleType                                                                                  # Import DoubleType for defining column types in Spark DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1A5Bs7cd8fr"
      },
      "outputs": [],
      "source": [
        "patient_demographics_path = \"/content/datasets/patient_demographics.csv\"                  #file path for patient_demographics dataset\n",
        "patient_visits_path = \"/content/datasets/patient_visits.csv\"                              #file path for patient_visits dataset\n",
        "hospital_path = \"/content/datasets/hospital_logs.xml\"                                     #file path for hospital_path dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYudAp6L6RY1"
      },
      "source": [
        "**Creating spark session**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJdDuEW9eq4I"
      },
      "outputs": [],
      "source": [
        "def create_spark_session(app_name=\"BigDataPipeline\", packages=\"com.databricks:spark-xml_2.12:0.15.0\"):\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(app_name) \\\n",
        "        .config(\"spark.jars.packages\", packages) \\\n",
        "        .getOrCreate()\n",
        "    return spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIHE_Tew6cI5"
      },
      "source": [
        "**Data Preprocessing for Patient Demographics Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXpCG1cefKv6"
      },
      "outputs": [],
      "source": [
        "def preprocess_demographics(spark, patient_demographics_path):\n",
        "    # Load demographics data using the specified file path variable\n",
        "    df_demo = spark.read.csv(patient_demographics_path, header=True, inferSchema=True)\n",
        "\n",
        "    # Fill numeric nulls: Compute the mean income and replace missing values for \"avg_monthly_income\"\n",
        "    income_mean = df_demo.select(mean(\"avg_monthly_income\")).first()[0]\n",
        "    df_demo = df_demo.fillna({\"avg_monthly_income\": income_mean})\n",
        "\n",
        "    # Fill categorical nulls: Replace missing values in specified categorical columns with \"Unknown\"\n",
        "    categorical_demo = [\"gender\", \"insurance\", \"marital_status\", \"education_level\", \"employment_status\", \"language_preference\"]\n",
        "    for c in categorical_demo:\n",
        "        df_demo = df_demo.fillna({c: \"Unknown\"})\n",
        "\n",
        "    # Standardize gender entries to a consistent format\n",
        "    df_demo = df_demo.withColumn(\"gender\",\n",
        "        when(col(\"gender\").isin(\"M\", \"Male\"), \"Male\")\n",
        "        .when(col(\"gender\").isin(\"F\", \"Female\"), \"Female\")\n",
        "        .otherwise(\"Other\")\n",
        "    )\n",
        "\n",
        "    # Cap the number of chronic conditions at a maximum value of 5\n",
        "    df_demo = df_demo.withColumn(\"chronic_conditions\", when(col(\"chronic_conditions\") > 5, 5).otherwise(col(\"chronic_conditions\")))\n",
        "\n",
        "    return df_demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7njoMFqa76PQ",
        "outputId": "2f6f8efc-0421-4587-db08-68d99e6dc9d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---+------+-------+---------+--------------+---------------+-----------------+------------------+--------------+-------------------+------------------+\n",
            "|patient_id|age|gender|zipcode|insurance|marital_status|education_level|employment_status|chronic_conditions|has_mobile_app|language_preference|avg_monthly_income|\n",
            "+----------+---+------+-------+---------+--------------+---------------+-----------------+------------------+--------------+-------------------+------------------+\n",
            "|         1| 51| Other|  49372| Medicare|        Single|       Bachelor|          Student|                 0|           Yes|              Hindi|           20268.7|\n",
            "|         2| 14|Female|  84681|     None|        Single|    High School|       Unemployed|                 2|           Yes|            English|          15850.09|\n",
            "|         3| 71|Female|  48588|  Private|       Married|         Master|       Unemployed|                 3|           Yes|            English|          30295.22|\n",
            "|         4| 60|  Male|  14087| Medicare|       Married|           None|         Employed|                 2|           Yes|            English|           35175.2|\n",
            "|         5| 20|  Male|  60609| Medicare|       Widowed|         Master|         Employed|                 1|           Yes|              Hindi|          26776.51|\n",
            "+----------+---+------+-------+---------+--------------+---------------+-----------------+------------------+--------------+-------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark = create_spark_session()\n",
        "df_demo = preprocess_demographics(spark, patient_demographics_path)\n",
        "df_demo.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Eyv2Cep6t9z"
      },
      "source": [
        "**Data Preprocessing for Patient Visits Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOU5nldpfd46"
      },
      "outputs": [],
      "source": [
        "def preprocess_visits(spark, patient_visits_path):\n",
        "    # Load visits data from the specified file path variable\n",
        "    df_visits = spark.read.csv(patient_visits_path, header=True, inferSchema=True)\n",
        "\n",
        "    # Fill nulls for specific columns with default values\n",
        "    df_visits = df_visits.fillna({\n",
        "        \"total_spent\": 0,           # Set missing total spent to 0\n",
        "        \"visit_duration\": 0,        # Set missing visit duration to 0\n",
        "        \"visit_type\": \"Unknown\",    # Set missing visit type to \"Unknown\"\n",
        "        \"department\": \"Unknown\",    # Set missing department to \"Unknown\"\n",
        "        \"appointment_day\": \"Unknown\", # Set missing appointment day to \"Unknown\"\n",
        "        \"drop_off\": 0               # Set missing drop off values to 0\n",
        "    })\n",
        "\n",
        "    # Ensure that the drop_off column is cast to integer type\n",
        "    df_visits = df_visits.withColumn(\"drop_off\", col(\"drop_off\").cast(\"int\"))\n",
        "\n",
        "    return df_visits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZQCx6y29AxO",
        "outputId": "bab954af-5590-4d73-a810-8113ee8561fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------+-----------+---------------+----------+---------------+-----------+------------------+---------------+--------------+------------------+--------------------+--------+\n",
            "|patient_id|num_visits|total_spent|time_in_waiting|visit_type|appointment_day| department|satisfaction_score|doctor_assigned|visit_duration|prescription_given|followup_recommended|drop_off|\n",
            "+----------+----------+-----------+---------------+----------+---------------+-----------+------------------+---------------+--------------+------------------+--------------------+--------+\n",
            "|    266401|         3|     459.34|           23.5| Follow-up|        Tuesday|Dermatology|                 1|           D147|          16.1|               Yes|                  No|       0|\n",
            "|    306112|         5|     896.87|           37.2|       New|         Friday|      Ortho|                10|           D019|          23.3|               Yes|                  No|       0|\n",
            "|    277015|         4|    1073.94|           32.1| Follow-up|         Sunday| Cardiology|                 1|           D132|          18.2|               Yes|                 Yes|       1|\n",
            "|    783386|         3|     744.33|           33.4| Emergency|      Wednesday|Dermatology|                 7|           D200|          22.7|                No|                  No|       1|\n",
            "|     60376|         2|     242.69|           37.0| Emergency|         Monday| Pediatrics|                 6|           D101|          15.6|               Yes|                 Yes|       1|\n",
            "+----------+----------+-----------+---------------+----------+---------------+-----------+------------------+---------------+--------------+------------------+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_visits = preprocess_visits(spark, patient_visits_path)\n",
        "df_visits.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtDHjDWM65GJ"
      },
      "source": [
        "**Data Processing for Hospital Logs (XML Format)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk6so_AAgSsz"
      },
      "outputs": [],
      "source": [
        "def preprocess_logs(spark, hospital_path):\n",
        "    # Load logs data from an XML file using the specified row tag \"log\"\n",
        "    df_logs = spark.read.format(\"xml\") \\\n",
        "        .option(\"rowTag\", \"log\") \\\n",
        "        .load(hospital_path)\n",
        "\n",
        "    # Print the schema of the DataFrame to review the structure of the XML data\n",
        "    df_logs.printSchema()\n",
        "\n",
        "    # Display the first 5 rows with no truncation of the column values\n",
        "    df_logs.show(5, truncate=False)\n",
        "\n",
        "    # Cast \"patient_id\" column to integer and convert \"timestamp\" column to a proper timestamp type\n",
        "    df_logs = df_logs.withColumn(\"patient_id\", col(\"patient_id\").cast(\"int\")) \\\n",
        "                     .withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm\"))\n",
        "\n",
        "    return df_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49dv2RLK9Rde",
        "outputId": "0b846b78-0141-4d15-b522-7a1abe2bae88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- department: string (nullable = true)\n",
            " |-- event: string (nullable = true)\n",
            " |-- log_type: string (nullable = true)\n",
            " |-- patient_id: long (nullable = true)\n",
            " |-- staff_on_duty: string (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            "\n",
            "+-----------+------------------+--------+----------+-------------+-------------------+\n",
            "|department |event             |log_type|patient_id|staff_on_duty|timestamp          |\n",
            "+-----------+------------------+--------+----------+-------------+-------------------+\n",
            "|Dermatology|Procedure         |info    |423602    |S030         |2023-04-13 20:27:00|\n",
            "|Ortho      |Checked-In        |critical|139717    |S088         |2023-05-09 13:04:00|\n",
            "|Dermatology|Checked-In        |critical|564794    |S054         |2023-11-16 16:42:00|\n",
            "|Pediatrics |Missed Appointment|info    |643004    |S088         |2023-04-21 12:45:00|\n",
            "|Ortho      |Procedure         |info    |159674    |S044         |2023-12-05 06:06:00|\n",
            "+-----------+------------------+--------+----------+-------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_logs = preprocess_logs(spark, hospital_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YaiDle07V7o"
      },
      "source": [
        "**Data Integration: Merging Demographics, Visits, and Logs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ei3EzaGla5F",
        "outputId": "aff166b5-7ef5-407b-e9d6-82459f587ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---+------+-------+---------+--------------+---------------+-----------------+------------------+--------------+-------------------+------------------+----------+-----------+---------------+----------+---------------+-----------+------------------+---------------+--------------+------------------+--------------------+--------+---------+-------------+-------------+----------------------+\n",
            "|patient_id|age|gender|zipcode|insurance|marital_status|education_level|employment_status|chronic_conditions|has_mobile_app|language_preference|avg_monthly_income|num_visits|total_spent|time_in_waiting|visit_type|appointment_day| department|satisfaction_score|doctor_assigned|visit_duration|prescription_given|followup_recommended|drop_off|log_count|critical_logs|unique_events|most_recent_department|\n",
            "+----------+---+------+-------+---------+--------------+---------------+-----------------+------------------+--------------+-------------------+------------------+----------+-----------+---------------+----------+---------------+-----------+------------------+---------------+--------------+------------------+--------------------+--------+---------+-------------+-------------+----------------------+\n",
            "|         1| 51| Other|  49372| Medicare|        Single|       Bachelor|          Student|                 0|           Yes|              Hindi|           20268.7|         1|     640.73|           23.9|       New|       Saturday| Cardiology|                10|           D064|          17.0|               Yes|                  No|       0|        1|            0|            1|            Cardiology|\n",
            "|         3| 71|Female|  48588|  Private|       Married|         Master|       Unemployed|                 3|           Yes|            English|          30295.22|         2|     1365.7|           10.4| Follow-up|        Tuesday|Dermatology|                 7|           D060|          11.7|               Yes|                  No|       1|        3|            1|            3|             Neurology|\n",
            "|         6| 82|Female|  94411|     None|        Single|         Master|         Employed|                 1|           Yes|            English|          34209.03|         5|     565.98|           22.3|       New|        Tuesday| Pediatrics|                 4|           D148|          16.9|               Yes|                  No|       0|        2|            0|            2|               General|\n",
            "|         9| 74|  Male|  17043|  Private|       Married|    High School|          Retired|                 1|           Yes|            English|          15827.51|         8|    1220.46|           22.5|       New|        Tuesday|        ENT|                 9|           D069|          17.5|               Yes|                 Yes|       1|        0|            0|            0|               Unknown|\n",
            "|        12|  2|  Male|  25462| Medicare|       Married|    High School|         Employed|                 1|           Yes|            English|          27544.68|         1|     911.84|           28.4| Emergency|      Wednesday| Cardiology|                 1|           D110|          21.6|               Yes|                  No|       0|        0|            0|            0|               Unknown|\n",
            "+----------+---+------+-------+---------+--------------+---------------+-----------------+------------------+--------------+-------------------+------------------+----------+-----------+---------------+----------+---------------+-----------+------------------+---------------+--------------+------------------+--------------------+--------+---------+-------------+-------------+----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "df_demo = spark.read.csv(patient_demographics_path, header=True, inferSchema=True)\n",
        "df_visits = spark.read.csv(patient_visits_path, header=True, inferSchema=True)\n",
        "df_logs = spark.read.format(\"xml\").option(\"rowTag\", \"log\").load(hospital_path)\n",
        "\n",
        "# Preprocess XML for merging\n",
        "df_logs = df_logs.withColumn(\"patient_id\", col(\"patient_id\").cast(\"int\")) \\\n",
        "                 .withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm\"))\n",
        "\n",
        "# Create aggregated log features per patient\n",
        "df_log_features = df_logs.groupBy(\"patient_id\").agg(\n",
        "    count(\"*\").alias(\"log_count\"),\n",
        "    count(when(col(\"log_type\") == \"critical\", True)).alias(\"critical_logs\"),\n",
        "    countDistinct(\"event\").alias(\"unique_events\")\n",
        ")\n",
        "\n",
        "latest_logs = df_logs.groupBy(\"patient_id\").agg(spark_max(\"timestamp\").alias(\"latest_log_time\"))\n",
        "# Change 'timestamp' to 'latest_log_time' in the join condition\n",
        "latest_department = df_logs.join(latest_logs, on=[\"patient_id\"], how='inner') \\\n",
        "                           .filter(col(\"timestamp\") == col(\"latest_log_time\")) \\\n",
        "                           .select(\"patient_id\", \"department\") \\\n",
        "                           .withColumnRenamed(\"department\", \"most_recent_department\") \\\n",
        "                           .dropDuplicates([\"patient_id\"])\n",
        "\n",
        "# Merge log features with department info\n",
        "df_log_features = df_log_features.join(latest_department, on=\"patient_id\", how=\"left\")\n",
        "\n",
        "# Final fill for missing department\n",
        "df_log_features = df_log_features.fillna({\n",
        "    \"log_count\": 0,\n",
        "    \"critical_logs\": 0,\n",
        "    \"unique_events\": 0,\n",
        "    \"most_recent_department\": \"Unknown\"\n",
        "})\n",
        "\n",
        "# MERGE ALL\n",
        "\n",
        "# 1. Merge demographics and visits\n",
        "df_patient = df_demo.join(df_visits, on=\"patient_id\", how=\"inner\")\n",
        "\n",
        "# 2. Merge with XML-derived log features\n",
        "df_final = df_patient.join(df_log_features, on=\"patient_id\", how=\"left\") \\\n",
        "                     .fillna({\n",
        "                         \"log_count\": 0,\n",
        "                         \"critical_logs\": 0,\n",
        "                         \"unique_events\": 0,\n",
        "                         \"most_recent_department\": \"Unknown\"\n",
        "                     })\n",
        "\n",
        "# Final dataset ready\n",
        "df_final.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO4VzKRB7anP"
      },
      "source": [
        "**Saving the final csv file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfLWzuHlglvg"
      },
      "outputs": [],
      "source": [
        "df_final.toPandas().to_csv(\"output.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laCUKjMoGVdG"
      },
      "source": [
        "**Encoding & Scaling Categorical & Numerical Features respectively**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8fbZzVfk7GX",
        "outputId": "fb749c3e-4a40-47db-958a-363c0039a398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---------------------+-------------------------+-------------------------+---------------------+---------------------+----------------------+---------------------+-------------------------+--------------------+----------------------+--------------------+--------------+-----------------+----------------------+-----------------------+-------------------------+---------------------------+------------------+-----------------------+------------------+------------------------------+----------------------+\n",
            "|drop_off|age_scaled           |chronic_conditions_scaled|avg_monthly_income_scaled|num_visits_scaled    |total_spent_scaled   |time_in_waiting_scaled|visit_duration_scaled|satisfaction_score_scaled|log_count_scaled    |critical_logs_scaled  |unique_events_scaled|gender_encoded|insurance_encoded|marital_status_encoded|education_level_encoded|employment_status_encoded|language_preference_encoded|visit_type_encoded|appointment_day_encoded|department_encoded|most_recent_department_encoded|has_mobile_app_encoded|\n",
            "+--------+---------------------+-------------------------+-------------------------+---------------------+---------------------+----------------------+---------------------+-------------------------+--------------------+----------------------+--------------------+--------------+-----------------+----------------------+-----------------------+-------------------------+---------------------------+------------------+-----------------------+------------------+------------------------------+----------------------+\n",
            "|0       |[0.25136518282765263]|[-1.0995690494780743]    |[-1.216543143918441]     |[-1.1564720547725387]|[-0.5474972548062687]|[-0.13569057823814124]|[0.3999997710427158] |[1.5667198370841258]     |[0.707138181023271] |[-0.40819982357947676]|[0.7828655603939542]|(3,[2],[1.0]) |(4,[2],[1.0])    |(4,[1],[1.0])         |(5,[0],[1.0])          |(4,[2],[1.0])            |(4,[2],[1.0])              |(3,[0],[1.0])     |(7,[5],[1.0])          |(6,[4],[1.0])     |(7,[2],[1.0])                 |(2,[0],[1.0])         |\n",
            "|1       |[1.020611077279707]  |[1.6527079117636063]     |[0.03669450621227889]    |[-0.5793214312775671]|[3.079393516978414]  |[-1.8260928511951962] |[-0.6819846668867554]|[0.5220002405873025]     |[3.5360940313250215]|[2.0341726867310537]  |[3.793199490458733] |(3,[1],[1.0]) |(4,[0],[1.0])    |(4,[0],[1.0])         |(5,[2],[1.0])          |(4,[1],[1.0])            |(4,[0],[1.0])              |(3,[1],[1.0])     |(7,[1],[1.0])          |(6,[1],[1.0])     |(7,[4],[1.0])                 |(2,[0],[1.0])         |\n",
            "|0       |[1.4436963192283367] |[-0.18214339573084742]   |[0.5258905630154479]     |[1.1521304392073477] |[-0.9214576741214197]|[-0.3360345513293475] |[0.3795849703270651] |[-0.5227193559095208]    |[2.1216161061741463]|[-0.40819982357947676]|[2.2880325254263436]|(3,[1],[1.0]) |(4,[3],[1.0])    |(4,[1],[1.0])         |(5,[2],[1.0])          |(4,[0],[1.0])            |(4,[0],[1.0])              |(3,[0],[1.0])     |(7,[1],[1.0])          |(6,[3],[1.0])     |(7,[1],[1.0])                 |(2,[0],[1.0])         |\n",
            "+--------+---------------------+-------------------------+-------------------------+---------------------+---------------------+----------------------+---------------------+-------------------------+--------------------+----------------------+--------------------+--------------+-----------------+----------------------+-----------------------+-------------------------+---------------------------+------------------+-----------------------+------------------+------------------------------+----------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Categorical columns\n",
        "categorical_cols = [\n",
        "    \"gender\", \"insurance\", \"marital_status\", \"education_level\",\n",
        "    \"employment_status\", \"language_preference\", \"visit_type\",\n",
        "    \"appointment_day\", \"department\", \"most_recent_department\",\n",
        "    \"has_mobile_app\"\n",
        "]\n",
        "\n",
        "# Encode Categorical Columns\n",
        "for c in categorical_cols:\n",
        "    # String Indexing\n",
        "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\")\n",
        "    df_final = indexer.fit(df_final).transform(df_final)\n",
        "\n",
        "    # One-Hot Encoding\n",
        "    encoder = OneHotEncoder(inputCol=f\"{c}_index\", outputCol=f\"{c}_encoded\")\n",
        "    df_final = encoder.fit(df_final).transform(df_final)\n",
        "\n",
        "# Scale Numerical Columns Individually\n",
        "numerical_cols = [\n",
        "    \"age\", \"chronic_conditions\", \"avg_monthly_income\", \"num_visits\",\n",
        "    \"total_spent\", \"time_in_waiting\", \"visit_duration\",\n",
        "    \"satisfaction_score\", \"log_count\", \"critical_logs\", \"unique_events\"\n",
        "]\n",
        "\n",
        "for c in numerical_cols:\n",
        "    assembler = VectorAssembler(inputCols=[c], outputCol=f\"{c}_vec\")\n",
        "    df_final = assembler.transform(df_final)\n",
        "\n",
        "    scaler = StandardScaler(inputCol=f\"{c}_vec\", outputCol=f\"{c}_scaled\", withMean=True, withStd=True)\n",
        "    df_final = scaler.fit(df_final).transform(df_final)\n",
        "\n",
        "# Select Only Required Processed Columns for Modeling/Mining\n",
        "preprocessed_cols = [\"drop_off\"] + \\\n",
        "                    [f\"{c}_scaled\" for c in numerical_cols] + \\\n",
        "                    [f\"{c}_encoded\" for c in categorical_cols]\n",
        "\n",
        "df_preprocessed = df_final.select(preprocessed_cols)\n",
        "df_preprocessed.show(3, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u-6gjhXzv42"
      },
      "source": [
        "**Data Mining: Extracting patterns from the scaled & encoded features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-dYJ9SkoEGq"
      },
      "outputs": [],
      "source": [
        "# Columns to include: all scaled numerics + encoded categories\n",
        "clustering_features = [col for col in df_preprocessed.columns if col.endswith(\"_scaled\") or col.endswith(\"_encoded\")]\n",
        "\n",
        "# Assemble all into a single features vector for KMeans\n",
        "vec_assembler = VectorAssembler(inputCols=clustering_features, outputCol=\"features\")\n",
        "df_cluster_ready = vec_assembler.transform(df_preprocessed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNVMUnzWzTlX",
        "outputId": "13c325bb-3d9a-4575-9789-409ba3fcef55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-------+\n",
            "|drop_off|cluster|\n",
            "+--------+-------+\n",
            "|       0|      0|\n",
            "|       1|      2|\n",
            "|       0|      0|\n",
            "|       1|      1|\n",
            "|       0|      1|\n",
            "|       0|      0|\n",
            "|       0|      1|\n",
            "|       0|      0|\n",
            "|       0|      1|\n",
            "|       1|      0|\n",
            "+--------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize KMeans with k clusters (try 3 first, tune later)\n",
        "kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=3, seed=42)\n",
        "\n",
        "# Fit the model\n",
        "kmeans_model = kmeans.fit(df_cluster_ready)\n",
        "\n",
        "# Predict cluster assignments\n",
        "df_clustered = kmeans_model.transform(df_cluster_ready)\n",
        "\n",
        "# Show sample with clusters\n",
        "df_clustered.select(\"drop_off\", \"cluster\").show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqxBcMZ1zWSr",
        "outputId": "fc4d77e1-0e97-48b7-9426-e26f30ffa267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+\n",
            "|cluster| count|\n",
            "+-------+------+\n",
            "|      1|485061|\n",
            "|      2|122960|\n",
            "|      0|191979|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Count how many patients are in each cluster\n",
        "df_clustered.groupBy(\"cluster\").count().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSzoNumu1hV5"
      },
      "outputs": [],
      "source": [
        "# Convert 'cluster' into usable numeric column\n",
        "df_clustered = df_clustered.withColumnRenamed(\"cluster\", \"patient_segment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3PBG-J5Gt50"
      },
      "source": [
        "**Balancing the dataset  for modelling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmChacodq0zT"
      },
      "outputs": [],
      "source": [
        "# Split by class\n",
        "df_major = df_clustered.filter(col(\"drop_off\") == 0)\n",
        "df_minor = df_clustered.filter(col(\"drop_off\") == 1)\n",
        "\n",
        "# Undersample class 0\n",
        "df_major_sampled = df_major.sample(withReplacement=False, fraction=24000/559743, seed=42)\n",
        "\n",
        "# Combine and shuffle\n",
        "df_balanced = df_major_sampled.union(df_minor).orderBy(rand())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wufsTNsiE9wn"
      },
      "outputs": [],
      "source": [
        "ml_features = [c for c in df_balanced.columns if c.endswith(\"_scaled\") or c.endswith(\"_encoded\")] + [\"patient_segment\"]\n",
        "\n",
        "# Drop existing features column if present\n",
        "df_balanced = df_balanced.drop(\"features\")\n",
        "\n",
        "# Assemble\n",
        "assembler = VectorAssembler(inputCols=ml_features, outputCol=\"features\")\n",
        "df_ml_ready = assembler.transform(df_balanced)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmWvnE6VG4Yy"
      },
      "source": [
        "**Splitting & Applying ML Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFMP79JVxxbB"
      },
      "outputs": [],
      "source": [
        "train, test = df_ml_ready.randomSplit([0.7, 0.3], seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2tBlJgWIeJb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LmjP2YXhDGFR",
        "outputId": "772cdb72-49dd-435e-a3b4-feeb0f16e7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Model Evaluation (Balanced Dataset):\n",
            "Logistic Regression       AUC: 0.5022  Accuracy: 0.9104\n",
            "Random Forest             AUC: 0.5100  Accuracy: 0.9104\n",
            "Gradient Boosted Trees    AUC: 0.5675  Accuracy: 0.9105\n"
          ]
        }
      ],
      "source": [
        "# Disable codegen if using Colab\n",
        "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n",
        "\n",
        "auc_eval = BinaryClassificationEvaluator(labelCol=\"drop_off\", metricName=\"areaUnderROC\")\n",
        "acc_eval = MulticlassClassificationEvaluator(labelCol=\"drop_off\", metricName=\"accuracy\")\n",
        "\n",
        "results = []\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"drop_off\")\n",
        "lr_model = lr.fit(train)\n",
        "lr_preds = lr_model.transform(test)\n",
        "results.append((\"Logistic Regression\", auc_eval.evaluate(lr_preds), acc_eval.evaluate(lr_preds)))\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"drop_off\", numTrees=50)\n",
        "rf_model = rf.fit(train)\n",
        "rf_preds = rf_model.transform(test)\n",
        "results.append((\"Random Forest\", auc_eval.evaluate(rf_preds), acc_eval.evaluate(rf_preds)))\n",
        "\n",
        "# GBT\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"drop_off\", maxIter=50)\n",
        "gbt_model = gbt.fit(train)\n",
        "gbt_preds = gbt_model.transform(test)\n",
        "results.append((\"Gradient Boosted Trees\", auc_eval.evaluate(gbt_preds), acc_eval.evaluate(gbt_preds)))\n",
        "\n",
        "# Output\n",
        "print(\" Model Evaluation (Balanced Dataset):\")\n",
        "for name, auc, acc in results:\n",
        "    print(f\"{name:<25} AUC: {auc:.4f}  Accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drUVeWMlHBfL"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4yOZENw6rHJP"
      },
      "outputs": [],
      "source": [
        "# Extract only the probability of class 1\n",
        "@udf(returnType=DoubleType())\n",
        "def extract_prob(prob_vector):\n",
        "    return float(prob_vector[1])  # probability of drop_off = 1\n",
        "\n",
        "# Add column with extracted probability\n",
        "gbt_preds_export = gbt_preds.withColumn(\"prob_dropoff\", extract_prob(\"probability\"))\n",
        "\n",
        "# Select relevant columns\n",
        "final_preds = gbt_preds_export.select(\"drop_off\", \"prediction\", \"prob_dropoff\", \"patient_segment\")\n",
        "\n",
        "# Export to CSV\n",
        "final_preds.write.csv(\"drop_off_predictions.csv\", header=True, mode=\"overwrite\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ugNzGZ7ySLI4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}